#!/usr/bin/env python3
"""
Example task demonstrating ForeverVM integration with the multi-agent implementation workflow.
This script shows how the Implementation Agent can generate and test code safely.
"""

import os
import sys
import json
from pathlib import Path

# Add parent directory to path for importing integrate_forevervm
parent_dir = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(parent_dir))

from integrate_forevervm import MultiAgentExecutor

# Sample implementation code generated by the Implementation Agent
SAMPLE_CODE = """
import requests
from bs4 import BeautifulSoup
import json
import os
from typing import Dict, List, Any, Optional

class WebsiteCrawler:
    """Website crawler for extracting structure and content."""
    
    def __init__(self, base_url: str):
        """Initialize the crawler with a base URL."""
        self.base_url = base_url
        self.visited_urls = set()
        self.site_map = {}
    
    def normalize_url(self, url: str) -> str:
        """Normalize URL for consistent tracking."""
        # Remove trailing slash
        url = url.rstrip('/')
        
        # Add base URL if relative path
        if not url.startswith('http'):
            if url.startswith('/'):
                url = self.base_url + url
            else:
                url = f"{self.base_url}/{url}"
        
        return url
    
    def extract_page_data(self, url: str) -> Dict[str, Any]:
        """Extract data from a webpage."""
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract page title
            title = soup.title.text if soup.title else "No Title"
            
            # Extract meta description
            meta_desc = ""
            meta_tag = soup.find("meta", attrs={"name": "description"})
            if meta_tag and "content" in meta_tag.attrs:
                meta_desc = meta_tag["content"]
            
            # Extract main content
            main_content = soup.find("main")
            if not main_content:
                main_content = soup.find("div", {"id": "content"})
            if not main_content:
                main_content = soup.find("div", {"class": "content"})
            
            content_text = main_content.text.strip() if main_content else ""
            
            # Extract links
            links = []
            for a_tag in soup.find_all("a", href=True):
                href = a_tag["href"]
                if href.startswith("#") or href.startswith("mailto:"):
                    continue
                    
                links.append({
                    "url": self.normalize_url(href),
                    "text": a_tag.text.strip() or "No Text"
                })
            
            return {
                "url": url,
                "title": title,
                "meta_description": meta_desc,
                "content_summary": content_text[:500] + "..." if len(content_text) > 500 else content_text,
                "links": links
            }
        
        except Exception as e:
            return {
                "url": url,
                "error": str(e)
            }
    
    def crawl(self, max_pages: int = 10) -> Dict[str, Any]:
        """Crawl the website starting from the base URL."""
        to_visit = [self.base_url]
        
        while to_visit and len(self.visited_urls) < max_pages:
            current_url = to_visit.pop(0)
            
            if current_url in self.visited_urls:
                continue
                
            print(f"Crawling: {current_url}")
            page_data = self.extract_page_data(current_url)
            
            self.visited_urls.add(current_url)
            self.site_map[current_url] = page_data
            
            # Add new links to visit
            if "links" in page_data:
                for link in page_data["links"]:
                    if link["url"] not in self.visited_urls and link["url"] not in to_visit:
                        to_visit.append(link["url"])
        
        return {
            "base_url": self.base_url,
            "pages_crawled": len(self.visited_urls),
            "site_map": self.site_map
        }
    
    def save_site_map(self, output_file: str) -> None:
        """Save the site map to a JSON file."""
        with open(output_file, 'w') as f:
            json.dump({
                "base_url": self.base_url,
                "pages_crawled": len(self.visited_urls),
                "site_map": self.site_map
            }, f, indent=2)
        
        print(f"Site map saved to {output_file}")

# Example usage
if __name__ == "__main__":
    # Get URL from command line or use default
    url = sys.argv[1] if len(sys.argv) > 1 else "https://example.com"
    
    # Create crawler and crawl the site
    crawler = WebsiteCrawler(url)
    result = crawler.crawl(max_pages=5)
    
    # Save the results
    output_dir = os.path.dirname(os.path.abspath(__file__))
    output_file = os.path.join(output_dir, "site_map.json")
    crawler.save_site_map(output_file)
    
    print(f"Crawling complete. Processed {result['pages_crawled']} pages.")
"""

# Sample test code generated by the Implementation Agent
SAMPLE_TEST_CODE = """
import unittest
from unittest.mock import patch, MagicMock
import json
import os

# Test the WebsiteCrawler class
class TestWebsiteCrawler(unittest.TestCase):
    
    def setUp(self):
        self.crawler = WebsiteCrawler("https://example.com")
    
    def test_normalize_url(self):
        # Test absolute URL
        self.assertEqual(
            self.crawler.normalize_url("https://example.com/page"), 
            "https://example.com/page"
        )
        
        # Test trailing slash
        self.assertEqual(
            self.crawler.normalize_url("https://example.com/page/"), 
            "https://example.com/page"
        )
        
        # Test relative URL with leading slash
        self.assertEqual(
            self.crawler.normalize_url("/page"), 
            "https://example.com/page"
        )
        
        # Test relative URL without leading slash
        self.assertEqual(
            self.crawler.normalize_url("page"), 
            "https://example.com/page"
        )
    
    @patch('requests.get')
    def test_extract_page_data(self, mock_get):
        # Mock response
        mock_response = MagicMock()
        mock_response.text = '''
        <html>
            <head>
                <title>Test Page</title>
                <meta name="description" content="Test description">
            </head>
            <body>
                <main>Main content here</main>
                <a href="/page1">Link 1</a>
                <a href="https://example.com/page2">Link 2</a>
                <a href="#section">Section link</a>
                <a href="mailto:test@example.com">Email</a>
            </body>
        </html>
        '''
        mock_response.raise_for_status = MagicMock()
        mock_get.return_value = mock_response
        
        # Test extraction
        result = self.crawler.extract_page_data("https://example.com")
        
        self.assertEqual(result["title"], "Test Page")
        self.assertEqual(result["meta_description"], "Test description")
        self.assertTrue("Main content here" in result["content_summary"])
        
        # Should have 2 valid links (excluding # and mailto:)
        self.assertEqual(len(result["links"]), 2)
        
        # Check links
        link_urls = [link["url"] for link in result["links"]]
        self.assertIn("https://example.com/page1", link_urls)
        self.assertIn("https://example.com/page2", link_urls)
    
    @patch('requests.get')
    def test_crawl(self, mock_get):
        # Mock response
        mock_response = MagicMock()
        mock_response.text = '''
        <html>
            <head><title>Test Page</title></head>
            <body>
                <main>Test content</main>
                <a href="/page1">Link 1</a>
            </body>
        </html>
        '''
        mock_response.raise_for_status = MagicMock()
        mock_get.return_value = mock_response
        
        # Test crawl with max_pages=1
        result = self.crawler.crawl(max_pages=1)
        
        self.assertEqual(result["pages_crawled"], 1)
        self.assertEqual(len(result["site_map"]), 1)
        self.assertIn("https://example.com", result["site_map"])

if __name__ == '__main__':
    unittest.main()
"""

def save_sample_files():
    """Save the sample code and test files."""
    output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output")
    os.makedirs(output_dir, exist_ok=True)
    
    # Save implementation code
    impl_file = os.path.join(output_dir, "website_crawler.py")
    with open(impl_file, "w") as f:
        f.write(SAMPLE_CODE)
    
    # Save test code
    test_file = os.path.join(output_dir, "test_website_crawler.py")
    with open(test_file, "w") as f:
        f.write(SAMPLE_TEST_CODE)
    
    return impl_file, test_file

def main():
    """Main function demonstrating ForeverVM integration."""
    print("üîÑ Starting ForeverVM integration example...")
    
    # Save sample files
    impl_file, test_file = save_sample_files()
    print(f"‚úÖ Created sample implementation file: {impl_file}")
    print(f"‚úÖ Created sample test file: {test_file}")
    
    # Create executor
    print("\nüõ†Ô∏è Initializing ForeverVM sandbox...")
    
    # Check for API token
    if not os.environ.get("FOREVERVM_API_TOKEN"):
        print("‚ö†Ô∏è FOREVERVM_API_TOKEN not set!")
        print("For demonstration purposes, we'll continue with a mock execution.")
        
        # Mock success results
        impl_result = {
            "success": True,
            "output": "Crawling: https://example.com\nSite map saved to output/site_map.json\nCrawling complete. Processed 1 pages.",
            "return_value": None,
            "error": None
        }
        
        test_result = {
            "success": True,
            "output": "Ran 3 tests in 0.123s\n\nOK",
            "return_value": None,
            "error": None
        }
    else:
        # Real execution using ForeverVM
        try:
            executor = MultiAgentExecutor()
            
            # Execute implementation code
            print("\nüß™ Executing implementation code in isolated sandbox...")
            impl_result = executor.safe_execute_implementation("website_crawler.py")
            
            # Execute test code
            print("\nüß™ Running tests in isolated sandbox...")
            test_result = executor.test_implementation("website_crawler.py", 
                                                       open(test_file, "r").read())
        except Exception as e:
            print(f"‚ùå Error: {str(e)}")
            return
    
    # Display results
    print("\n--- Implementation Execution Results ---")
    if impl_result["success"]:
        print("‚úÖ Implementation code executed successfully")
        print("\nOutput:")
        print(impl_result["output"])
    else:
        print("‚ùå Implementation execution failed")
        print("\nError:")
        print(impl_result["error"])
    
    print("\n--- Test Execution Results ---")
    if test_result["success"]:
        print("‚úÖ Tests executed successfully")
        print("\nOutput:")
        print(test_result["output"])
    else:
        print("‚ùå Test execution failed")
        print("\nError:")
        print(test_result["error"])
    
    print("\nüéâ ForeverVM integration example complete!")
    print("This demonstrates how ForeverVM enables safe execution of:")
    print("1. Implementation code generated by AI agents")
    print("2. Test code to validate implementations")
    print("3. Package installation in isolated environments")
    print("\nAll code execution happened in a secure sandbox, isolated from your system.")

if __name__ == "__main__":
    main()